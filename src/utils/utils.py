'''
@Author: Xiao Tong
@FileName: utils.py
@CreateTime: 2022-03-22 20:51:50
@Description:

'''

import os
import pickle
import logging
import datetime
import numpy as np
from gensim.utils import tokenize
from collections import OrderedDict


def getLogger(log_dir: str, name: str="log") -> logging.Logger:
    """
    @param: log_dir: directory to save log files
    @param: name: logger name
    """
    logger = logging.Logger(name=name, level=logging.INFO)
    if not os.path.isdir(log_dir):
        os.makedirs(log_dir)

    now = datetime.datetime.now()
    filename = "HARNN_{}".format(now.strftime("%Y-%m-%d_%H:%M:%S"))
    formatter = logging.Formatter(fmt="%(asctime)s - %(filename)s - %(levelname)s - %(message)s", datefmt="%Y-%m-%d %H:%M:%S")
    file_handler = logging.FileHandler(os.path.join(log_dir, filename), mode="w", encoding="utf-8")
    stream_handler = logging.StreamHandler()
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    stream_handler.setLevel(logging.INFO)
    stream_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)
    return logger


def get_stopwords(stopwords_file: str) -> list:
    stopwords = []
    with open(stopwords_file, "r", encoding="utf-8") as fin:
        for line in fin:
            word = line.rstrip("\n")
            stopwords.append(word)
    return stopwords


def load_word2vec_pretrained(glove_file: str):
    """
    load pretrained word2vec embedding from GloVe
    """
    dirname, basename = os.path.split(glove_file)
    prefix = ".".join(basename.split(".")[:-1])
    word2vec_savefile = os.path.join(dirname, prefix + ".word2vec.pkl")
    if os.path.isfile(word2vec_savefile):
        word2vec = pickle.load(open(word2vec_savefile, "rb"))
        word2idx, pretrained_embedding = word2vec["word2idx"], word2vec["embedding"]
        return word2idx, pretrained_embedding

    word2idx = OrderedDict()
    pretrained_embedding = []
    with open(glove_file, "r", encoding="utf-8") as fin:
        for idx, line in enumerate(fin):
            splited_line = line.strip().split()
            word, vector = splited_line[0], splited_line[1:]
            vector = [float(num) for num in vector]
            word2idx[word] = idx
            pretrained_embedding.append(vector)
        pretrained_embedding = np.array(pretrained_embedding, dtype=np.float32)
    word2vec = {"word2idx": word2idx, "embedding": pretrained_embedding}
    pickle.dump(word2vec, open(word2vec_savefile, "wb"))
    return word2idx, pretrained_embedding


def tokenize_and_pad(subtitles: str, stopwords: list, word2idx: dict, pad_word: str, max_seq_len: int):
    """
    tokenize subtitles & embedding words & padding to max_seq_len
    """
    # tokenize
    subtitles = [list(tokenize(subtitle, lowercase=True, deacc=True)) for subtitle in subtitles]
    # get rid of stopwords & embedding words
    subtitles = [[word2idx.get(word, word2idx["<unk>"]) for word in subtitle if word not in stopwords] for subtitle in subtitles]
    lens = [min(len(subtitle), max_seq_len) for subtitle in subtitles]
    for idx, length in enumerate(lens):
        if length >= max_seq_len:
            subtitles[idx] = subtitles[idx][:max_seq_len]
        else:
            subtitles[idx] = subtitles[idx] + [word2idx[pad_word] for _ in range(max_seq_len - length)]
    return subtitles, lens


def load_data_by_path(filepath: str, stopwords: list, word2idx: dict, pad_word: str, max_seq_len: int):
    """
    load single sample file, extract images, subtitles and finish embedding at the same time.
    """
    data = pickle.load(open(filepath, "rb"))
    images, subtitles = data["keyframes"], data["subtitles"]
    lens = []
    for idx, section_subtitles in enumerate(subtitles):
        section_subtitles, section_lens = tokenize_and_pad(section_subtitles, stopwords, word2idx, pad_word=pad_word, max_seq_len=max_seq_len)
        subtitles[idx] = section_subtitles
        lens.append(section_lens)
    return images, subtitles, lens


def load_khan_data(config: dict, word2idx: dict, pad_word: str="<unk>"):
    """
    load data from `*.keyframes.pkl` which are generated by `utils/sample_keyframe.py`
    """
    if pad_word not in word2idx:
        raise KeyError("'{0}' is not in word2idx!".format(pad_word))

    sample_dir = os.path.join(config["data"]["root_dir"], config["data"]["sample_dir"])
    stopwords_filepath = os.path.join(config["data"]["root_dir"], config["data"]["stopwords_file"])
    stopwords = get_stopwords(stopwords_filepath)

    images, subtitles, lens = [], [], []
    for fileid in os.listdir(sample_dir):
        sample_file = os.path.join(sample_dir, fileid, fileid + ".keyframes.pkl")
        local_images, local_subtitles, local_lens = load_data_by_path(sample_file, stopwords, word2idx, pad_word, config["model"]["max_seq_len"])
        images.append(local_images)
        subtitles.append(local_subtitles)
        lens.append(local_lens)
    return images, subtitles, lens


if __name__ == "__main__":
    sample_dir = "data/samples"
    import yaml
    config = yaml.load(open("config/model.train.yaml", "r"), Loader=yaml.FullLoader)
    word2idx, _ = load_word2vec_pretrained(config["data"]["word2vec"])
    load_khan_data(config, word2idx)
